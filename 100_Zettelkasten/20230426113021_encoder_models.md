
---
creation date: 2023-04-26 11:30
last updated: 2023-04-26 11:30
---
# [[20230426113021_encoder_models]] - Encoder models
__Tags__: #deep-learning  #encoder

---
__Contents__: Attention layers can access the all the words in the initial sentence. This is called **bi-directional** attention (or auto-encoding model). The pretraining consists in computing a given sentence and tasking the model to reconstruct the initial sentence.

These models can be used to do sentence classification, named entity recognition, extractive question answering, ...

__References__:
