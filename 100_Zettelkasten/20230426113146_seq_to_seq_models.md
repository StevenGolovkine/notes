
---
creation date: 2023-04-26 11:31
last updated: 2023-04-26 11:31
---
# [[20230426113146_seq_to_seq_models]] - Sequence to sequence models
__Tags__: #deep-learning #seq-to-seq

---
__Contents__: Attention layers in the encoder can access to all the words in the sentence while in the decoder, they can only access to previous words. The pretraining consists of a combinaison of encoder and decoder models.

These models can be used to do text summarization, translation, generative question answering, ...

__References__:


