---

---

---
creation date: 2023-06-20 09:17
last updated: 2023-06-20 09:17
---
# [[20230620091722_unigram]] - Unigram tokenization
__Tags__: #algorithm-theory  #tokenizers 

---
__Contents__:
The Unigram tokenizer is used in SentencePiece. It starts from a big vocabulary and removes tokens until the desired vocabulary size (the base vocabulary can be created using BPE ([[20230620091612_byte_pair_encoding]]) on the corpus with large vocabulary size). The choice of the removing a symbol from the vocabulary is based on a loss function (base character are never removed).

$$loss = - \sum_{w \in W} \log(\mathbb{P}(w))$$
It is assumed that each token is independent from the previous one (not good to generate text). The probability of a given token is its frequency divided by the sum of all frequencies. We can use the Viterbi algorithm to compute the probabilities as its creates a graph to detect the possible segmentation of a given word.

__References__:



