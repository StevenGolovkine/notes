
---
creation date: 2023-04-12 14:20
last updated: 2023-04-12 14:20
---
# [[20230412142013_transformers]] - Transformers
__Tags__: #natural-language-processing  #large-language-model

---
__Contents__: Transformers models have been introduced in 2017 by Google for translation ([[20230426112405_transformers_translation]]). These models are large language models. They are trained in a self-supervised way (no human needed to label the data). It gets a statistical understanding of the language (pretrained). Then, using transfer learning to fine tune the model in a supervised way (human in the loop). Open-source Python library exists ([[20230426114600_transformers_library]]).

Transformers are big models. Larger size (more parameters) and more data lead to better model performance.

Models that used transformers:
* Auto-regressive models (GPT and derivatives)
* Auto-encoding models (BERT and derivatives)
* Sequences-to-sequences models (BART/T5)

The architecture of transformers is based on two blocks: one encoder and one decoder. Both can be independent. If there is only an encoder ([[20230426113021_encoder_models]]]], the goal is to understand the input. If there is only a decoder ([[20230426113101_decoder_models]]), the goal is to do generative tasks. For sequence-to-sequence models ([[20230426113146_seq_to_seq_models]]), the goal is also generative but considering the input.

The blocks are based on **attention layers**. The idea is to pay specific attention to certain words and ignore the others (words have meaning and the meaning is affected by the context). 

Some vocabulary:
* Architecture: skeleton of a model (layers and operations)
* Checkpoints: weights that will be loaded for a given architecture
* Model: encompasse architecture and checkpoints

Pay attention that all models have bias and limitations. All pretrained models thus come with bias, mostly because researchers scrape all the data they can find. Be careful, that fine-tuning the model will not make bias disappear.



__References__:

