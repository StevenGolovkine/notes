
---
creation date: 2023-06-20 09:16
last updated: 2023-06-20 09:16
---
# [[20230620091655_wordpiece]] - WordPiece tokenization
__Tags__: #algorithm-theory  #tokenizers 

---
__Contents__:
The WordPiece tokenizer is used to pretrain BERT models. It is based on the same steps as the BPE tokenizer ([[20230620091612_byte_pair_encoding]]) but with a different way to learn the merges. Merging is based on a score defined as 
$$score = \# pair / (\# 1st \times \# 2nd).$$
It will prioritizes the pairs where the individuals are not frequent.
Contrary to BPE, it only saves the final vocabulary and not the merge rules and can classify vocabulary as unknown and not only individual character. 

__References__:



