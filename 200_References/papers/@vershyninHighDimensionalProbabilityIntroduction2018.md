---
category: literaturenote
tags: data-science-theory, probability-theory
citekey: vershyninHighDimensionalProbabilityIntroduction2018
itemType: book
status: unread  
dateread:  
---

> [!Cite]  
> Vershynin, Roman. 2018. _High-Dimensional Probability: An Introduction with Applications in Data Science: 47_. 1st edition. Cambridge New York, NY Port Melbourne, VIC New Delhi Singapore: Cambridge University Press.

> [!SYNTHESIS] 
>**Contribution**::
>
>**Related**:: 
>

> [!METADATA]  
>
**Author**:: Vershynin, Roman<br>
> **Title**:: High-Dimensional Probability: An Introduction with Applications in Data Science: 47    
> **Year**:: 2018     
> **Citekey**:: @vershyninHighDimensionalProbabilityIntroduction2018    
>    
>    
>     
>    
>**Publisher**:: Cambridge University Press    
>**Location**:: Cambridge New York, NY Port Melbourne, VIC New Delhi Singapore     
>    
>    
>**ISBN**:: 978-1-108-41519-4

> [!LINK] 
>
> [Vershynin_2018_High-Dimensional Probability.pdf](file:///Users/steven/Library/CloudStorage/GoogleDrive-steven.golovkine@ul.ie/My%20Drive/bibliography/Cambridge%20University%20Press/2018/Vershynin_2018_High-Dimensional%20Probability.pdf).

>[!Abstract]
>
>High-dimensional probability offers insight into the behavior of random vectors, random matrices, random subspaces, and objects used to quantify uncertainty in high dimensions. Drawing on ideas from probability, analysis, and geometry, it lends itself to applications in mathematics, statistics, theoretical computer science, signal processing, optimization, and more. It is the first to integrate theory, key tools, and modern applications of high-dimensional probability. Concentration inequalities form the core, and it covers both classical results such as Hoeffding's and Chernoff's inequalities and modern developments such as the matrix Bernstein's inequality. It then introduces the powerful methods based on stochastic processes, including such tools as Slepian's, Sudakov's, and Dudley's inequalities, as well as generic chaining and bounds based on VC dimension. A broad range of illustrations is embedded throughout, including classical and modern results for covariance estimation, clustering, networks, semidefinite programming, coding, dimension reduction, matrix completion, machine learning, compressed sensing, and sparse regression.
>>


# Notes<br>
# Annotations%% begin annotations %%  
 
  
%% end annotations %%

%% Import Date: 2023-10-27T21:07:15.495+01:00 %%
