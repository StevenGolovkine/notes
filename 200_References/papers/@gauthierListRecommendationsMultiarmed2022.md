---
category: literaturenote
tags: bandits-theory, markov-process, online-learning, recommendation-systems
citekey: gauthierListRecommendationsMultiarmed2022
itemType: thesis
status: unread  
dateread:  
---

> [!Cite]  
> Gauthier, Camille-Sovanneary. 2022. “List Recommendations with Multi-Armed Bandits.” These de doctorat, Rennes 1. [https://www.theses.fr/2022REN1S023](https://www.theses.fr/2022REN1S023).

> [!SYNTHESIS] 
>**Contribution**::
>
>**Related**:: 
>

> [!METADATA]  
>
**Author**:: Gauthier, Camille-Sovanneary
**Contributor**:: Fromont, Élisa
**Contributor**:: Gaudel, Romaric<br>
> **Title**:: List recommendations with multi-armed bandits    
> **Year**:: 2022     
> **Citekey**:: @gauthierListRecommendationsMultiarmed2022    
>    
>    
>     
>    
>    
>     
>    
>    
>

> [!LINK] 
>
> [Gauthier_2022_List recommendations with multi-armed bandits.pdf](file:///Users/steven/Library/CloudStorage/GoogleDrive-steven.golovkine@ul.ie/My%20Drive/bibliography/Rennes%201/2022/Gauthier_2022_List%20recommendations%20with%20multi-armed%20bandits.pdf).

>[!Abstract]
>
>Nous étudions le problème d'apprentissage de l'ordonnancement en ligne de L items pour K positions prédéfinies sur une page web. Pour cela, nous nous intéressons aux algorithmes de bandits manchots qui apprennent les paramètres de modèles de clics identifiés, tel que le modèle basé sur les positions (PBM). Les algorithmes de l'état-de-l'art s'attaquent rarement au PBM complet, où tous les paramètres sont inconnus. De plus, l'état de l'art contient peu d'algorithmes basés sur Thompson Sampling ou sur les bandits unimodaux, malgré leurs performances empiriques reconnues. Nos deux premières contributions s'appuient sur les bandits unimodaux : GRAB est spécialisé pour un PBM complet et UniRank, traite des modèles de clics divers. Ces deux contributions, très efficaces, ont une borne supérieure de regret théorique. La troisième contribution fournit une famille de bandits adressant le problème PBM complet en couplant l'algorithme Thompson Sampling avec des méthodes d'échantillonnage par   chaînes de Markov Monte-Carlo (MCMC). Deux méthodes MCMC sont utilisées : par descente de gradient par Langevin,  donnant des résultats empiriques semblables à l'état de l'art avec un temps de calcul bas et stable, et par Metropolis Hasting, qui offre le regret empirique le plus bas pour ce problème pour un PBM complet.
>>


# Notes
Sous la direction de Élisa Fromont et de Romaric Gaudel. Soutenue le 17-03-2022,à Rennes 1 , dans le cadre de MATHSTIC , en partenariat avec Institut de recherche en informatique et systèmes aléatoires (Rennes) (laboratoire) , Institut de recherche en informatique et systèmes aléatoires (Rennes) (laboratoire) et de LACODAM (laboratoire) .<br>
# Annotations%% begin annotations %%  
 
  
%% end annotations %%

%% Import Date: 2023-10-24T12:00:47.616+01:00 %%
